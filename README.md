# LGL-learning
LGLはCVPR2019年に発表されたHao Chengらの論文 [Local to Global Learning: Gradually Adding Classes for Training Deep Neural Networks] にて提案された、Deep Neural Network(DNN)における新たな学習パラダイムである。 <br>

論文URL: https://openaccess.thecvf.com/content_CVPR_2019/papers/Cheng_Local_to_Global_Learning_Gradually_Adding_Classes_for_Training_Deep_CVPR_2019_paper.pdf

## 論文について
### 直観的な話
私たちはものを学習することにおいて、全てを覚えるのではなく、まず単純なものから学びそこから様々なものを覚えていく。
(例えば、子供が犬という言葉と姿を知り、これも犬？と聞きながら猫を覚えていく。) <br>
この直感に基づいて「簡単なサンプルから学習を始め、徐々に学習するサンプルを複雑にしていく」学習手法、Curriculum Learning(CL)が2009年にBengioらによって提案された。
(参考：Evaluating capability of deep neural networks for image classification via information plane. )

### SPL
Self-Paced Learning(SPL)は、上記のCLを基に学習カリキュラムを自動的に生成して学習を行う手法で、この論文において比較対象とされる手法である。<br>
CL法では学習する「簡単なサンプル」から「複雑なサンプル」まで、手動で設定する必要があるが、SPL法では損失関数の値に基づいて各トレーニングイテレーションで「簡単なサンプル」を抽出して、それを学習する。
しかし、この抽出において新たなハイパーパラメータが必要となり、そのチューニングの手間がかかるという問題点がある。

### LGL
SPLの問題やCLの手動設定の問題を解決するために、LGLでは以下のように「簡単なサンプル」から「複雑なサンプル」までを学習するカリキュラムを提案した。<br>
1. 全サンプルから一部サンプルをランダム抽出して「簡単なサンプル」として学習する
1. その学習サンプルの学習を一定行ったあと、残ったサンプルから一定数のサンプルを抽出して「簡単なサンプル」に加える
1. 2を繰り返して最終的には全サンプルを学習する（「複雑なサンプル」の学習)
<br>
Haoらはこの手法を一般物体認識問題に適用させた学習を行い、その評価を行った。<br>
具体的には以下の手順である: <br>

1. 全サンプルをラベルごとに分類し、いくつかのラベルの学習データを「簡単なサンプル」にする
1. 残ったラベルの学習データからいくつかのラベルを抽出し、その学習データを「簡単なサンプル」に追加する
1. 最終的に全データをトレーニングデータとした学習を行う

この次のラベルの抽出において
ランダム抽出・学習データのラベルに”似ている”ラベルを抽出・学習データのラベルに”似ていない”ラベルを抽出
という３種類の抽出手法を挙げられたが、性能において大きな差は見られなかった。

### 論文の新規性・重要性

